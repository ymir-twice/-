{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************optimizer_1*********************\n",
      "optimizer_1.defaults： {'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "optimizer_1.param_groups长度： 1\n",
      "optimizer_1.param_groups一个元素包含的键： dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused'])\n",
      "\n",
      "******************optimizer_2*********************\n",
      "optimizer_2.defaults： {'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "optimizer_2.param_groups长度： 1\n",
      "optimizer_2.param_groups一个元素包含的键： dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused'])\n",
      "\n",
      "******************optimizer_3*********************\n",
      "optimizer_3.defaults： {'lr': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "optimizer_3.param_groups长度： 2\n",
      "optimizer_3.param_groups一个元素包含的键： dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools\n",
    "\n",
    "initial_lr = 0.1\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "net_1 = model()\n",
    "net_2 = model()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\n",
    "print(\"******************optimizer_1*********************\")\n",
    "print(\"optimizer_1.defaults：\", optimizer_1.defaults)\n",
    "print(\"optimizer_1.param_groups长度：\", len(optimizer_1.param_groups))\n",
    "print(\"optimizer_1.param_groups一个元素包含的键：\", optimizer_1.param_groups[0].keys())\n",
    "print()\n",
    "\n",
    "optimizer_2 = torch.optim.Adam([*net_1.parameters(), *net_2.parameters()], lr = initial_lr)\n",
    "# optimizer_2 = torch.opotim.Adam(itertools.chain(net_1.parameters(), net_2.parameters())) # 和上一行作用相同\n",
    "print(\"******************optimizer_2*********************\")\n",
    "print(\"optimizer_2.defaults：\", optimizer_2.defaults)\n",
    "print(\"optimizer_2.param_groups长度：\", len(optimizer_2.param_groups))\n",
    "print(\"optimizer_2.param_groups一个元素包含的键：\", optimizer_2.param_groups[0].keys())\n",
    "print()\n",
    "\n",
    "optimizer_3 = torch.optim.Adam([{\"params\": net_1.parameters()}, {\"params\": net_2.parameters()}], lr = initial_lr)\n",
    "print(\"******************optimizer_3*********************\")\n",
    "print(\"optimizer_3.defaults：\", optimizer_3.defaults)\n",
    "print(\"optimizer_3.param_groups长度：\", len(optimizer_3.param_groups))\n",
    "print(\"optimizer_3.param_groups一个元素包含的键：\", optimizer_3.param_groups[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = next(net_1.parameters())\n",
    "type(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.named_parameters()\n",
    "返回值是一个生成器，构成的迭代元素是一个元组。包含两个元素：(name.layer, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "conv2.weight\n",
      "conv2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, _ in net_1.named_parameters(): \n",
    "    print(name)  # 果然是这样子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaLR(opeimizer, lr_lambda, last_epoch)\n",
    " 1. optimizer: 要更改的学习率的优化器\n",
    " 2. lr_lambda (function of list): 根据epoch计算$\\lambda$的函数，返回值一个存全部$\\lambda$的列表。分别计算各个parameter groups 的学习率更新用到的 $\\lambda$\n",
    " 3. last_epoch(int): 最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认-1表示从头开始训练，即从epoch=1开始\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 0.1\n",
      "第1个epoch的学习率：0.100000\n",
      "第2个epoch的学习率：0.050000\n",
      "第3个epoch的学习率：0.033333\n",
      "第4个epoch的学习率：0.025000\n",
      "第5个epoch的学习率：0.020000\n",
      "第6个epoch的学习率：0.016667\n",
      "第7个epoch的学习率：0.014286\n",
      "第8个epoch的学习率：0.012500\n",
      "第9个epoch的学习率：0.011111\n",
      "第10个epoch的学习率：0.010000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "initial_lr = 0.1\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "net_1 = model()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)\n",
    "scheduler_1 = LambdaLR(optimizer_1, lr_lambda=lambda epoch: 1/(epoch+1))\n",
    "\n",
    "print(\"初始化的学习率：\", optimizer_1.defaults['lr'])\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    # train\n",
    "\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_1.step()\n",
    "    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer_1.param_groups[0]['lr']))\n",
    "    scheduler_1.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim.lr_scheduler.StepLR\n",
    "**更新策略:**\n",
    "\n",
    "每过step_size个epoch，做一次更新：$$new_lr = initial_lr \\times \\gamma^{epoch // step_size}$$\n",
    "其中new_lr是得到的新的学习率，initial_lr是初始化的学习率，step_size是参数step_size，$\\gamma$是参数$\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率:  0.1\n",
      "第1个epoch的学习率: 0.100000\n",
      "第2个epoch的学习率: 0.100000\n",
      "第3个epoch的学习率: 0.100000\n",
      "第4个epoch的学习率: 0.010000\n",
      "第5个epoch的学习率: 0.010000\n",
      "第6个epoch的学习率: 0.010000\n",
      "第7个epoch的学习率: 0.001000\n",
      "第8个epoch的学习率: 0.001000\n",
      "第9个epoch的学习率: 0.001000\n",
      "第10个epoch的学习率: 0.000100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import itertools\n",
    "\n",
    "initial_lr = 0.1\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "net_1 = model()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(net_1.parameters(), lr=initial_lr)\n",
    "scheduler_1 = StepLR(optimizer_1, step_size=3, gamma=0.1)\n",
    "\n",
    "print(\"初始化的学习率: \", optimizer_1.defaults['lr'])\n",
    "for epoch in range(1, 11):\n",
    "    # train\n",
    "\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_1.step()\n",
    "    print(\"第%d个epoch的学习率: %f\"%(epoch, optimizer_1.param_groups[0]['lr']))\n",
    "    scheduler_1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import itertools\n",
    "\n",
    "initial_lr = 0.1\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "net_1 = model()\n",
    "\n",
    "optimizer_1 = torch.optim.Adam(net_1.parameters(), lr=initial_lr)\n",
    "scheduler_1 = MultiStepLR(optimizer_1, )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gssai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
