{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用如下两句可以保持import模块的更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random,os\n",
    "import numpy as np\n",
    "def set_random_seed(seed: int) -> None:\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 37\n",
    "set_random_seed(seed)\n",
    "class AxialDW(nn.Module):\n",
    "    def __init__(self, dim, mixer_kernel, dilation = 1):\n",
    "        super().__init__()\n",
    "        h, w = mixer_kernel\n",
    "        self.dw_h = nn.Conv2d(dim, dim, kernel_size=(h, 1), padding='same', groups = dim, dilation = dilation)\n",
    "        self.dw_w = nn.Conv2d(dim, dim, kernel_size=(1, w), padding='same', groups = dim, dilation = dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dw_h(x) + self.dw_w(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Encoding then downsampling\"\"\"\n",
    "    def __init__(self, in_c, out_c, mixer_kernel = (7, 7)):\n",
    "        super().__init__()\n",
    "        self.dw = AxialDW(in_c, mixer_kernel = (7, 7))\n",
    "        self.bn = nn.BatchNorm2d(in_c)\n",
    "        self.pw = nn.Conv2d(in_c, out_c, kernel_size=1)\n",
    "        self.down = nn.MaxPool2d((2,2))\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = self.bn(self.dw(x))\n",
    "        x = self.act(self.down(self.pw(skip)))\n",
    "        return x, skip\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Upsampling then decoding\"\"\"\n",
    "    def __init__(self, in_c, out_c, mixer_kernel = (7, 7), size = False):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "        self.size = size\n",
    "            # self.up = nn.ConvTranspose2d(\n",
    "            #             in_channels=256,\n",
    "            #             out_channels=256,\n",
    "            #             kernel_size=5,  \n",
    "            #             stride=2, \n",
    "            #             padding=1  \n",
    "            #         \n",
    "\n",
    "        self.pw = nn.Conv2d(in_c + out_c, out_c,kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.dw = AxialDW(out_c, mixer_kernel = (7, 7))\n",
    "        self.act = nn.GELU()\n",
    "        self.pw2 = nn.Conv2d(out_c, out_c, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        if(self.size):\n",
    "            x = F.interpolate(x, size=(25, 25), mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        # x = F.interpolate(x, size=(25, 25), mode='bilinear', align_corners=True)\n",
    "        x = self.act(self.pw2(self.dw(self.bn(self.pw(x)))))\n",
    "\n",
    "        return x\n",
    "\n",
    "class BottleNeckBlock(nn.Module):\n",
    "    \"\"\"Axial dilated DW convolution\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        gc = dim//4\n",
    "        self.pw1 = nn.Conv2d(dim, gc, kernel_size=1)\n",
    "        self.dw1 = AxialDW(gc, mixer_kernel = (3, 3), dilation = 1)\n",
    "        self.dw2 = AxialDW(gc, mixer_kernel = (3, 3), dilation = 2)\n",
    "        self.dw3 = AxialDW(gc, mixer_kernel = (3, 3), dilation = 3)\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(4*gc)\n",
    "        self.pw2 = nn.Conv2d(4*gc, dim, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw1(x)\n",
    "        x = torch.cat([x, self.dw1(x), self.dw2(x), self.dw3(x)], 1)\n",
    "        x = self.act(self.pw2(self.bn(x)))\n",
    "        return x\n",
    "\n",
    "class self_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        self.conv_in = nn.Conv2d(3, 16, kernel_size=7, padding='same')\n",
    "        self.e1 = EncoderBlock(16, 32)\n",
    "        self.e2 = EncoderBlock(32, 64)\n",
    "        self.e3 = EncoderBlock(64, 128)\n",
    "        self.e4 = EncoderBlock(128, 256)\n",
    "        self.e5 = EncoderBlock(256, 512)\n",
    "\n",
    "        \"\"\"Bottle Neck\"\"\"\n",
    "        self.b5 = BottleNeckBlock(512)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        self.d5 = DecoderBlock(512, 256)\n",
    "        self.d4 = DecoderBlock(256, 128, size = True)\n",
    "        self.d3 = DecoderBlock(128, 64)\n",
    "        self.d2 = DecoderBlock(64, 32)\n",
    "        self.d1 = DecoderBlock(32, 16)\n",
    "        self.conv_out = nn.Conv2d(16, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Encoder\"\"\"\n",
    "        x = self.conv_in(x)\n",
    "        x, skip1 = self.e1(x)\n",
    "        x, skip2 = self.e2(x)\n",
    "        x, skip3 = self.e3(x)\n",
    "        x, skip4 = self.e4(x)\n",
    "        x, skip5 = self.e5(x)\n",
    "\n",
    "        \"\"\"BottleNeck\"\"\"\n",
    "        x = self.b5(x)          # 512 6 6\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        x = self.d5(x, skip5)   # 256 12 12\n",
    "        x = self.d4(x, skip4)\n",
    "        x = self.d3(x, skip3)\n",
    "        x = self.d2(x, skip2)\n",
    "        x = self.d1(x, skip1)\n",
    "        x = self.conv_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据\n",
    "这里需要先将我们的数据集做成便于torch读取的格式。但是出于方便考虑，就先使用最基本的Dataset重载来实现了\n",
    "\n",
    "#### **后续优化此处也许可以实现FPS性能的提升**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预处理机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# 定义图像和掩码的预处理\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.Lambda(lambda x: torch.tensor(np.array(x), dtype=torch.long)) \n",
    "])\n",
    "\n",
    "# 创建数据集对象\n",
    "trainset = myDataset(idx_path='stats/train-meta.csv', img_dir='data/images/training/', mask_dir='data/annotations/training/', transform=image_transform, mask_transform=mask_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "### 超参数定义\n",
    "总共有一下几个超参数需要定义: \n",
    " - 优化算法\n",
    " - 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = self_net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "weight_decay = 5e-3\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, nesterov=True, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr, betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loss_function.dice_loss import DiceLoss\n",
    "criterion = DiceLoss(weights=[0.68,1.5,0.81,1])#10 20 12 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练的主循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[38;2;192;255;32m          \u001b[0m| 0/280 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/1000, total loss 0.00000:   0%|\u001b[38;2;192;255;32m          \u001b[0m| 1/280 [00:26<2:03:09, 26.49s/it, loss=1.05, lr=0.000333]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, lr\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m800\u001b[39m:\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mULite_tmp.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.lr_scheduler import WarmupMultiStepLR, WarmupCosineLR\n",
    "\n",
    "num_epochs = 1000\n",
    "total_loss = []\n",
    "epoch_loss = 0\n",
    "milestones = [50,100,150,200,250]\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    pbar = tqdm(trainloader, colour='#C0FF20')\n",
    "    total_batchs = len(trainloader)\n",
    "    pbar.set_description(f'{epoch}/{num_epochs}, total loss {epoch_loss:.5f}')\n",
    "    scheduler = WarmupCosineLR(optimizer, T_max=num_epochs + 1, last_epoch=epoch - 2, warmup_factor=1.0 / 3, warmup_iters=200)    # 有热身的cos loss \n",
    "    #scheduler = WarmupMultiStepLR(optimizer,milestones=milestones,gamma=0.7,warmup_factor=1.0 / 3,warmup_iters=300)\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (inputs, gts) in enumerate(pbar):\n",
    "        inputs, gts = inputs.to(device), gts.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, gts)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if i == 200 or i == 500 or i == 800:\n",
    "        torch.save(model, \"ULite_tmp.pth\")\n",
    "    scheduler.step()\n",
    "    total_loss.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'models/.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化模型结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import utils.vis_result as vst\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6)) \n",
    "fig.legend(handles=vst.legend_patches, loc='upper right', bbox_to_anchor=(0.65, 1.05), ncol=4)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "vst.plot_result(model, np.random.choice(vst.trainmeta['id']), True, axes[0], True)\n",
    "vst.plot_result(model, np.random.choice(vst.testmeta['id']), False, axes[1], True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gssai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
